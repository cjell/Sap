# NepalRAG  
**Multimodal Retrieval-Augmented Generation for Plant Identification and Reasoning**

NepalRAG is a **multimodal retrieval-augmented generation (RAG) pipeline** designed for **image-based plant identification and ecological reasoning**, with a focus on Nepalese flora.  
The system integrates **visual embeddings, language-based descriptions, and structured metadata** to produce grounded, explainable responses based on retrieved evidence rather than free-form generation.

---

## Project Overview

The goal of NepalRAG is to enable accurate and context-aware reasoning over plant data by combining:

- **Plant images**
- **LLM-generated textual descriptions**
- **Structured botanical metadata** (e.g., species, origin, usage)

Rather than relying on a single modality, the system performs **independent visual and textual retrieval** and fuses results to improve robustness and recall.

---

## Architecture Summary

The pipeline is built around **three primary databases**:

1. **Image Database**  
   Raw plant images embedded using a vision foundation model.

2. **Text Description Database**  
   Natural-language descriptions of each image generated by a multimodal LLM and embedded into a text vector space.

3. **Metadata Database**  
   Structured plant information (species, origin, usage, etc.) linked to both image and text entries.

---

## Models & Representations

### Vision Encoding
- **DINOv2-Base (Facebook AI)**  
  Used to generate dense vector embeddings for plant images.  
  DINOv2 provides strong semantic representations without task-specific fine-tuning, making it well-suited for open-world visual retrieval.

### Image-to-Text Description
- **LLaVA-Next (7B parameters)**  
  Converts plant images into rich textual descriptions that capture visual characteristics relevant for retrieval and reasoning.

### Text Embedding
- **SentenceTransformer: `intfloat/e5-base-v2`**  
  Embeds LLaVA-generated descriptions into a semantic vector space optimized for retrieval.

---

## Vector Storage

- **FAISS (Facebook AI Similarity Search)** is used to store and query:
  - Image embeddings (DINOv2)
  - Text embeddings (SentenceTransformer)

Each modality is indexed **independently**, allowing flexible retrieval and fusion strategies.

---

## Query & Retrieval Pipeline

When a **query image** is provided:

1. The image is embedded using **DINOv2**
2. The image is passed to **LLaVA-Next**, producing a textual description
3. The description is embedded using **SentenceTransformer**
4. Both embeddings are queried against their respective **FAISS** indices
5. Results are merged using **Reciprocal Rank Fusion (RRF)**

This produces a unified, ranked list of the most relevant candidates across modalities.

---

## Fusion Strategy

- **Reciprocal Rank Fusion (RRF)**  
  Combines rankings from the image and text retrieval pipelines without requiring score normalization.  
  This improves robustness when one modality performs better than the other.

---

## Generation Stage

The final stage passes the following to the generation model:

- Top-k fused retrieval candidates
- Associated metadata (species, origin, usage, etc.)
- The original query image

**GPT-5** then generates the final response, grounded explicitly in retrieved content rather than unconstrained generation.

---

## Current Status

- Multimodal ingestion pipeline implemented
- Independent visual and textual retrieval working
- RRF-based fusion validated
- End-to-end generation functional

---

## Future Work

- Expanded dataset coverage
- Improved metadata normalization
- Multilingual support (Nepali â†” English)
- Formal evaluation of retrieval and generation faithfulness
- User-facing interface for researchers and practitioners

---

## Disclaimer

This system is intended for **research and educational use**.  
Outputs should not replace expert botanical or ecological judgment.

---

## References

- Riedler et al., *Beyond Text: Multimodal Retrieval-Augmented Generation*
- Oquab et al., *DINOv2: Learning Robust Visual Features without Supervision*
- Liu et al., *Improved Baselines with LLaVA-Next*
- Cormack et al., *Reciprocal Rank Fusion*
