import os
from uuid import uuid4
from typing import Optional, List, Dict, Any
from fastapi.middleware.cors import CORSMiddleware
from fastapi import UploadFile, File
from fastapi.responses import StreamingResponse
import io

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

from openai import OpenAI

from PIL import Image

from .router import Router
from .memory import MemoryStore
from .utils import decode_base64_image, extract_text_field

load_dotenv()
client = OpenAI()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GPT_MODEL = os.getenv("GPT_MODEL", "gpt-4.1-mini")

if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY not set in .env")


app = FastAPI(title="Sap — Nepal Plant Multimodal RAG")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class QueryRequest(BaseModel):
    text: Optional[str] = None
    image_base64: Optional[str] = None
    session_id: Optional[str] = None


class RetrievedItem(BaseModel):
    id: str
    source: Optional[str] = None
    text: str
    score: Optional[float] = None
    rrf_score: Optional[float] = None
    extra: Dict[str, Any] = {}


class QueryResponse(BaseModel):
    session_id: str
    mode: str
    caption: Optional[str]
    answer: str
    retrieved: List[RetrievedItem]



print("-Loading Router, Memory, Models, FAISS-")
router = Router()
memory = MemoryStore()
print("-Everything is Loaded-\n")


def call_gpt(messages: List[Dict[str, str]]) -> str:

    response = client.chat.completions.create(
        model=GPT_MODEL,
        messages=messages,
        temperature=0.2,
        max_tokens=300,
    )

    return response.choices[0].message.content

# Endpoint for frontend to hit
@app.post("/query", response_model=QueryResponse)
def query(req: QueryRequest):

    if not req.text and not req.image_base64:
        raise HTTPException(400, "Provide text and/or image_base64")

    session_id = req.session_id or str(uuid4())
    user_text = req.text.strip() if req.text else None

    pil_image: Optional[Image.Image] = None
    if req.image_base64:
        try:
            pil_image = decode_base64_image(req.image_base64)
        except Exception as e:
            raise HTTPException(400, f"Invalid base64 image: {e}")

    route_out = router.handle_query(
        text=user_text,
        image=pil_image,
        top_k=5,
    )

    mode = route_out.get("mode", "text")
    caption = route_out.get("generated_caption")
    fused_all = route_out.get("fused_ranked", []) or []
    identified_plant = route_out.get("identified_plant")

    fused = fused_all[:3]

  
    plant_candidate: Optional[Dict[str, Any]] = None
    if identified_plant:
        plant_candidate = identified_plant
    else:
        for item in fused_all:
            if item.get("source") == "plant_metadata":
                plant_candidate = item
                break

    context_blocks: List[str] = []

    if plant_candidate:
        pname = (
            plant_candidate.get("plant_name")
            or plant_candidate.get("name")
            or plant_candidate.get("plant_id")
        )
        ptext = plant_candidate.get("text") or extract_text_field(plant_candidate)
        context_blocks.append(
            f"Identified plant candidate: {pname or 'Unknown'}\n"
            f"Plant details: {ptext}"
        )

    for idx, item in enumerate(fused):
        if plant_candidate is not None and item is plant_candidate:
            continue
        textval = extract_text_field(item)
        context_blocks.append(f"[{idx+1}] ({item.get('source')}) {textval}")

    context_str = "\n\n".join(context_blocks) if context_blocks else "No retrieved context."

    if user_text:
        question = user_text
    elif caption:
        question = "Identify and describe this plant based on the image caption."
    else:
        question = "Help the user with their plant-related request."

    system_msg = {
        "role": "system",
        "content": (
            "You are SAP (this is your name, say it if asked what you are or what your name is), a friendly and knowledgeable plant-identification, "
            "agricultural, ecological,and ethnobotany assistant. You behave like a normal conversational agent "
            "but with special expertise in Nepalese plants. You may use internal "
            "context (retrieved text, image caption) but NEVER mention them or imply "
            "that they came from a machine. Always speak naturally to the user."
            "If the user asks about plant/ecological topics, you may answer, but steer towards your specialty."
            "Do not use special formatting, lists, bullets, or latex."
        ),
    }
    internal_context_msg = {
        "role": "assistant",
        "content": (
            f"[INTERNAL CONTEXT – DO NOT REVEAL]\n"
            f"Image understanding: {caption or 'No image'}\n"
            f"Plant candidate: {plant_candidate or 'None'}\n"
            f"Retrieved knowledge:\n{context_str}\n"
        )
}

    past = memory.get(session_id)

    user_msg = {
        "role": "user",
        "content": user_text or "What plant is this?",
    }

    messages = [system_msg] + past + [internal_context_msg, user_msg]

    gpt_answer = call_gpt(messages)

    memory.append(session_id, "user", question)
    memory.append(session_id, "assistant", gpt_answer)

    retrieved_clean: List[RetrievedItem] = []
    for item in fused:
        textval = extract_text_field(item)
        extra = {
            k: v
            for k, v in item.items()
            if k not in {"id", "source", "faiss_distance", "rrf_score"}
        }
        retrieved_clean.append(
            RetrievedItem(
                id=str(item.get("id")),
                source=item.get("source"),
                text=textval,
                score=float(item.get("faiss_distance", 0.0)),
                rrf_score=float(item.get("rrf_score", 0.0)),
                extra=extra,
            )
        )

    return QueryResponse(
        session_id=session_id,
        mode=mode,
        caption=caption,
        answer=gpt_answer,
        retrieved=retrieved_clean,
    )

@app.post("/stt")
async def stt_endpoint(file: UploadFile = File(...)):
    audio_bytes = await file.read()

    transcript = client.audio.transcriptions.create(
        model="whisper-1",
        file=("audio.wav", audio_bytes)
    )

    return {"text": transcript.text}

class TTSRequest(BaseModel):
    text: str

@app.post("/tts")
async def tts_endpoint(req: TTSRequest):
    audio = client.audio.speech.create(
        model="gpt-4o-mini-tts",
        voice="alloy",
        input=req.text,
    )

    audio_bytes = audio.read()

    return StreamingResponse(
        io.BytesIO(audio_bytes),
        media_type="audio/wav"
    )